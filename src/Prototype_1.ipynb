{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\ARI5121_NLP\\75_Assignment_1\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import string\n",
    "#\n",
    "BASE_DIR = os.path.join( os.path.dirname(os.getcwd()))\n",
    "print(BASE_DIR)\n",
    "train_path = BASE_DIR + \"\\\\data\\\\ssec-aggregated\\\\train-combined-0.0.csv\"\n",
    "test_path = BASE_DIR + \"\\\\data\\\\ssec-aggregated\\\\test-combined-0.0.csv\"\n",
    "# Emotion Categories (Classes)\n",
    "emotions = (\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"trust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training corpus: \n",
      "(2912,)\n",
      "\n",
      "First 5 lines of training corpus: \n",
      "['Anger\\tAnticipation\\tDisgust\\tFear\\t---\\tSadness\\tSurprise\\t---\\t@tedcruz And  #HandOverTheServer she wiped clean + 30k deleted emails  explains dereliction of duty/lies re #Benghazi etc #tcot    ', '---\\tAnticipation\\t---\\tFear\\tJoy\\t---\\t---\\tTrust\\tHillary is our best choice if we truly want to continue being a progressive nation. #Ohio       ', \"Anger\\tAnticipation\\tDisgust\\t---\\tJoy\\tSadness\\t---\\t---\\t@TheView I think our country is ready for a female pres  it can't ever be Hillary      \", \"Anger\\tAnticipation\\tDisgust\\tFear\\t---\\tSadness\\t---\\t---\\tI just gave an unhealthy amount of my hard-earned money away to the big gov't & untrustworthy IRS. #WhyImNotVotingForHillary       \", '---\\t---\\t---\\t---\\tJoy\\t---\\t---\\t---\\t@PortiaABoulger Thank you for adding me to your list       ']\n"
     ]
    }
   ],
   "source": [
    "def open_file_and_return_dump(path):\n",
    "    \"\"\" \n",
    "    Opens file and returns a dump containing all the data.\n",
    "    The data structure used to record the dumped data is a list of strings,\n",
    "    where each string represents a single record).\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    with open(path) as csvfile:\n",
    "        rows = csv.reader(csvfile)\n",
    "        for row in rows:\n",
    "            temp_string = \"\"\n",
    "            for item in row:\n",
    "                temp_string += str(item) + \" \"\n",
    "            corpus.append(temp_string)\n",
    "    return corpus\n",
    "#\n",
    "corpus = open_file_and_return_dump(path=train_path)\n",
    "#\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(corpus)) + \"\\n\")\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(corpus[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of training corpus: \n",
      "['@tedcruz And  #HandOverTheServer she wiped clean + 30k deleted emails  explains dereliction of duty/lies re #Benghazi etc #tcot    ', 'Hillary is our best choice if we truly want to continue being a progressive nation. #Ohio       ', \"@TheView I think our country is ready for a female pres  it can't ever be Hillary      \", \"I just gave an unhealthy amount of my hard-earned money away to the big gov't & untrustworthy IRS. #WhyImNotVotingForHillary       \", '@PortiaABoulger Thank you for adding me to your list       ']\n",
      "\n",
      "First 5 lines of training corpus: \n",
      "['Anger\\tAnticipation\\tDisgust\\tFear\\t---\\tSadness\\tSurprise\\t---', '---\\tAnticipation\\t---\\tFear\\tJoy\\t---\\t---\\tTrust', 'Anger\\tAnticipation\\tDisgust\\t---\\tJoy\\tSadness\\t---\\t---', 'Anger\\tAnticipation\\tDisgust\\tFear\\t---\\tSadness\\t---\\t---', '---\\t---\\t---\\t---\\tJoy\\t---\\t---\\t---']\n",
      "\n",
      "Shape of training corpus: \n",
      "(2912,)\n",
      "Shape of training corpus: \n",
      "(2912,)\n"
     ]
    }
   ],
   "source": [
    "def split_labels_from_vectors(corpus):\n",
    "    \"\"\"\n",
    "    Takes corpus as input, and splits the data into respective labels and training data.\n",
    "    Returns this data as two separate lists\n",
    "    \"\"\"\n",
    "    #\n",
    "    label_count = len(emotions)\n",
    "    character_splitter = '\\t'\n",
    "    train_y, train_X = [], []\n",
    "    for row in corpus:\n",
    "        groups = row.split(character_splitter)\n",
    "        train_y.append(character_splitter.join(groups[:label_count]))\n",
    "        train_X.append(character_splitter.join(groups[label_count:]))\n",
    "    return train_y, train_X\n",
    "#\n",
    "train_y, train_X = split_labels_from_vectors(corpus=corpus)\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(train_X[0:5]) + \"\\n\")\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(train_y[0:5]) + \"\\n\")\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(train_X)))\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of training corpus: \n",
      "[['@tedcruz', 'And', '', '#HandOverTheServer', 'she', 'wiped', 'clean', '+', '30k', 'deleted', 'emails', '', 'explains', 'dereliction', 'of', 'duty/lies', 're', '#Benghazi', 'etc', '#tcot', '', '', '', ''], ['Hillary', 'is', 'our', 'best', 'choice', 'if', 'we', 'truly', 'want', 'to', 'continue', 'being', 'a', 'progressive', 'nation.', '#Ohio', '', '', '', '', '', '', ''], ['@TheView', 'I', 'think', 'our', 'country', 'is', 'ready', 'for', 'a', 'female', 'pres', '', 'it', \"can't\", 'ever', 'be', 'Hillary', '', '', '', '', '', ''], ['I', 'just', 'gave', 'an', 'unhealthy', 'amount', 'of', 'my', 'hard-earned', 'money', 'away', 'to', 'the', 'big', \"gov't\", '&', 'untrustworthy', 'IRS.', '#WhyImNotVotingForHillary', '', '', '', '', '', '', ''], ['@PortiaABoulger', 'Thank', 'you', 'for', 'adding', 'me', 'to', 'your', 'list', '', '', '', '', '', '', '']]\n",
      "\n",
      "First 5 lines of training corpus: \n",
      "[['Anger', 'Anticipation', 'Disgust', 'Fear', '---', 'Sadness', 'Surprise', '---'], ['---', 'Anticipation', '---', 'Fear', 'Joy', '---', '---', 'Trust'], ['Anger', 'Anticipation', 'Disgust', '---', 'Joy', 'Sadness', '---', '---'], ['Anger', 'Anticipation', 'Disgust', 'Fear', '---', 'Sadness', '---', '---'], ['---', '---', '---', '---', 'Joy', '---', '---', '---']]\n",
      "\n",
      "Shape of training corpus: \n",
      "(2912,)\n",
      "Shape of training corpus: \n",
      "(2912, 8)\n"
     ]
    }
   ],
   "source": [
    "def tokenize_corpus(data, delimeter=\" \"):\n",
    "    \"\"\"\n",
    "    Takes a list as input, and tokenizes the data.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    [(corpus.append(row.split(delimeter))) for row in data]\n",
    "    return corpus\n",
    "tokenized_train_X, tokenized_train_y = tokenize_corpus(data=train_X), tokenize_corpus(data=train_y, delimeter=\"\\t\")\n",
    "#\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(tokenized_train_X[0:5]) + \"\\n\")\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(tokenized_train_y[0:5]) + \"\\n\")\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(tokenized_train_X)))\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(tokenized_train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of training corpus: \n",
      "[['tedcruz', 'handovertheserver', 'wiped', 'clean', '30k', 'deleted', 'emails', 'explains', 'dereliction', 'dutylies', 'benghazi', 'etc', 'tcot'], ['hillary', 'best', 'choice', 'truly', 'want', 'continue', 'progressive', 'nation.', 'ohio'], ['theview', 'think', 'country', 'ready', 'female', 'pres', 'cant', 'ever', 'hillary'], ['gave', 'unhealthy', 'amount', 'hardearned', 'money', 'away', 'big', 'govt', 'untrustworthy', 'irs.', 'whyimnotvotingforhillary'], ['portiaaboulger', 'thank', 'adding', 'list']]\n",
      "\n",
      "Shape of training corpus: \n",
      "(2912,)\n"
     ]
    }
   ],
   "source": [
    "def _replace(word, symbols, placeholder=\"\"):\n",
    "    \"\"\"\n",
    "    An overriding of the original python method, so as to replace all characters \n",
    "    in a string based on whether they occur in a list.\n",
    "    \"\"\"\n",
    "    temp_string = word\n",
    "    for symbol in symbols:\n",
    "        temp_string = temp_string.replace(symbol, placeholder)\n",
    "    return temp_string\n",
    "#\n",
    "def remove_stop_words(data):\n",
    "    \"\"\"\n",
    "    Takes a list of data, and iterates over each element to \n",
    "    scan and remover stop words.\n",
    "    \n",
    "    The method ensures to convert all text instances to lowercase.\n",
    "    \n",
    "    The method ensures to remove jargon symbols (#,%,&,etc).\n",
    "    \n",
    "    This method ensures to remove punctuation.\n",
    "    \"\"\"\n",
    "    temp_list, jargon_symbols = [], ('%','$','#','@','^','&','*','(',')','+','-','/','\\'')\n",
    "    for row in data:\n",
    "        filtered_words = [_replace(word.lower().translate(string.punctuation), symbols=jargon_symbols) for word in row if word.lower() not in stopwords.words('english')]          \n",
    "        temp_list.append(filtered_words)\n",
    "    #\n",
    "    new_list = []\n",
    "    for row in temp_list:\n",
    "        row_list = []\n",
    "        for word in row:\n",
    "            if word != \"\":\n",
    "                row_list.append(word)\n",
    "        new_list.append(row_list)\n",
    "    #\n",
    "    return new_list\n",
    "#\n",
    "cleaned_tokenized_train_X = remove_stop_words(data=tokenized_train_X)\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(cleaned_tokenized_train_X[0:5]) + \"\\n\")\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(cleaned_tokenized_train_X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
