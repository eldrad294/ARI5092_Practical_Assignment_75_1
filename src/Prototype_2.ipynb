{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\ARI5121_NLP\\75_Assignment_1\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import string\n",
    "#\n",
    "BASE_DIR = os.path.join( os.path.dirname(os.getcwd()))\n",
    "print(BASE_DIR)\n",
    "train_path = BASE_DIR + \"\\\\data\\\\ssec-aggregated\\\\train-combined-0.0.csv\"\n",
    "test_path = BASE_DIR + \"\\\\data\\\\ssec-aggregated\\\\test-combined-0.0.csv\"\n",
    "# Emotion Categories (Classes)\n",
    "emotions = (\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"trust\")\n",
    "alpha = 1 # Laplace Smoothener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training corpus: \n",
      "(2912,)\n",
      "\n",
      "First 5 lines of training corpus: \n",
      "['Anger\\tAnticipation\\tDisgust\\tFear\\t---\\tSadness\\tSurprise\\t---\\t@tedcruz And  #HandOverTheServer she wiped clean + 30k deleted emails  explains dereliction of duty/lies re #Benghazi etc #tcot    ', '---\\tAnticipation\\t---\\tFear\\tJoy\\t---\\t---\\tTrust\\tHillary is our best choice if we truly want to continue being a progressive nation. #Ohio       ', \"Anger\\tAnticipation\\tDisgust\\t---\\tJoy\\tSadness\\t---\\t---\\t@TheView I think our country is ready for a female pres  it can't ever be Hillary      \", \"Anger\\tAnticipation\\tDisgust\\tFear\\t---\\tSadness\\t---\\t---\\tI just gave an unhealthy amount of my hard-earned money away to the big gov't & untrustworthy IRS. #WhyImNotVotingForHillary       \", '---\\t---\\t---\\t---\\tJoy\\t---\\t---\\t---\\t@PortiaABoulger Thank you for adding me to your list       ']\n"
     ]
    }
   ],
   "source": [
    "def open_file_and_return_dump(path):\n",
    "    \"\"\" \n",
    "    Opens file and returns a dump containing all the data.\n",
    "    The data structure used to record the dumped data is a list of strings,\n",
    "    where each string represents a single record).\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    with open(path) as csvfile:\n",
    "        rows = csv.reader(csvfile)\n",
    "        for row in rows:\n",
    "            temp_string = \"\"\n",
    "            for item in row:\n",
    "                temp_string += str(item) + \" \"\n",
    "            corpus.append(temp_string)\n",
    "    return corpus\n",
    "#\n",
    "corpus = open_file_and_return_dump(path=train_path)\n",
    "#\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(corpus)) + \"\\n\")\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(corpus[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of training corpus: \n",
      "['@tedcruz And  #HandOverTheServer she wiped clean + 30k deleted emails  explains dereliction of duty/lies re #Benghazi etc #tcot    ', 'Hillary is our best choice if we truly want to continue being a progressive nation. #Ohio       ', \"@TheView I think our country is ready for a female pres  it can't ever be Hillary      \", \"I just gave an unhealthy amount of my hard-earned money away to the big gov't & untrustworthy IRS. #WhyImNotVotingForHillary       \", '@PortiaABoulger Thank you for adding me to your list       ']\n",
      "\n",
      "First 5 lines of training corpus: \n",
      "['Anger\\tAnticipation\\tDisgust\\tFear\\t---\\tSadness\\tSurprise\\t---', '---\\tAnticipation\\t---\\tFear\\tJoy\\t---\\t---\\tTrust', 'Anger\\tAnticipation\\tDisgust\\t---\\tJoy\\tSadness\\t---\\t---', 'Anger\\tAnticipation\\tDisgust\\tFear\\t---\\tSadness\\t---\\t---', '---\\t---\\t---\\t---\\tJoy\\t---\\t---\\t---']\n",
      "\n",
      "Shape of training corpus: \n",
      "(2912,)\n",
      "Shape of training corpus: \n",
      "(2912,)\n"
     ]
    }
   ],
   "source": [
    "def split_labels_from_vectors(corpus):\n",
    "    \"\"\"\n",
    "    Takes corpus as input, and splits the data into respective labels and training data.\n",
    "    Returns this data as two separate lists\n",
    "    \"\"\"\n",
    "    #\n",
    "    label_count = len(emotions)\n",
    "    character_splitter = '\\t'\n",
    "    train_y, train_X = [], []\n",
    "    for row in corpus:\n",
    "        groups = row.split(character_splitter)\n",
    "        train_y.append(character_splitter.join(groups[:label_count]))\n",
    "        train_X.append(character_splitter.join(groups[label_count:]))\n",
    "    return train_y, train_X\n",
    "#\n",
    "train_y, train_X = split_labels_from_vectors(corpus=corpus)\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(train_X[0:5]) + \"\\n\")\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(train_y[0:5]) + \"\\n\")\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(train_X)))\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of training corpus: \n",
      "[['@tedcruz', 'And', '', '#HandOverTheServer', 'she', 'wiped', 'clean', '+', '30k', 'deleted', 'emails', '', 'explains', 'dereliction', 'of', 'duty/lies', 're', '#Benghazi', 'etc', '#tcot', '', '', '', ''], ['Hillary', 'is', 'our', 'best', 'choice', 'if', 'we', 'truly', 'want', 'to', 'continue', 'being', 'a', 'progressive', 'nation.', '#Ohio', '', '', '', '', '', '', ''], ['@TheView', 'I', 'think', 'our', 'country', 'is', 'ready', 'for', 'a', 'female', 'pres', '', 'it', \"can't\", 'ever', 'be', 'Hillary', '', '', '', '', '', ''], ['I', 'just', 'gave', 'an', 'unhealthy', 'amount', 'of', 'my', 'hard-earned', 'money', 'away', 'to', 'the', 'big', \"gov't\", '&', 'untrustworthy', 'IRS.', '#WhyImNotVotingForHillary', '', '', '', '', '', '', ''], ['@PortiaABoulger', 'Thank', 'you', 'for', 'adding', 'me', 'to', 'your', 'list', '', '', '', '', '', '', '']]\n",
      "\n",
      "First 5 lines of training corpus: \n",
      "[['Anger', 'Anticipation', 'Disgust', 'Fear', '---', 'Sadness', 'Surprise', '---'], ['---', 'Anticipation', '---', 'Fear', 'Joy', '---', '---', 'Trust'], ['Anger', 'Anticipation', 'Disgust', '---', 'Joy', 'Sadness', '---', '---'], ['Anger', 'Anticipation', 'Disgust', 'Fear', '---', 'Sadness', '---', '---'], ['---', '---', '---', '---', 'Joy', '---', '---', '---']]\n",
      "\n",
      "Shape of training corpus: \n",
      "(2912,)\n",
      "Shape of training corpus: \n",
      "(2912, 8)\n"
     ]
    }
   ],
   "source": [
    "def tokenize_corpus(data, delimeter=\" \"):\n",
    "    \"\"\"\n",
    "    Takes a list as input, and tokenizes the data.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    [(corpus.append(row.split(delimeter))) for row in data]\n",
    "    return corpus\n",
    "tokenized_train_X, tokenized_train_y = tokenize_corpus(data=train_X), tokenize_corpus(data=train_y, delimeter=\"\\t\")\n",
    "#\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(tokenized_train_X[0:5]) + \"\\n\")\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(tokenized_train_y[0:5]) + \"\\n\")\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(tokenized_train_X)))\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(tokenized_train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of training corpus: \n",
      "[['tedcruz', 'handovertheserver', 'wiped', 'clean', 'k', 'deleted', 'emails', 'explains', 'dereliction', 'dutylies', 'benghazi', 'etc', 'tcot'], ['hillary', 'best', 'choice', 'truly', 'want', 'continue', 'progressive', 'nation', 'ohio'], ['theview', 'think', 'country', 'ready', 'female', 'pres', 'cant', 'ever', 'hillary'], ['gave', 'unhealthy', 'amount', 'hardearned', 'money', 'away', 'big', 'govt', 'untrustworthy', 'irs', 'whyimnotvotingforhillary'], ['portiaaboulger', 'thank', 'adding', 'list']]\n",
      "\n",
      "First 5 lines of training corpus: \n",
      "[['anger', 'anticipation', 'disgust', 'fear', 'sadness', 'surprise'], ['anticipation', 'fear', 'joy', 'trust'], ['anger', 'anticipation', 'disgust', 'joy', 'sadness'], ['anger', 'anticipation', 'disgust', 'fear', 'sadness'], ['joy']]\n",
      "\n",
      "Shape of training corpus: \n",
      "(2912,)\n",
      "Shape of training corpus: \n",
      "(2912,)\n"
     ]
    }
   ],
   "source": [
    "def _replace(word, symbols, placeholder=\"\"):\n",
    "    \"\"\"\n",
    "    An overriding of the original python method, so as to replace all characters \n",
    "    in a string based on whether they occur in a list.\n",
    "    \"\"\"\n",
    "    temp_string = word\n",
    "    for symbol in symbols:\n",
    "        temp_string = temp_string.replace(symbol, placeholder)\n",
    "    return temp_string\n",
    "#\n",
    "def remove_stop_words(data):\n",
    "    \"\"\"\n",
    "    Takes a list of data, and iterates over each element to \n",
    "    scan and remover stop words.\n",
    "    \n",
    "    The method ensures to convert all text instances to lowercase.\n",
    "    \n",
    "    The method ensures to remove jargon symbols (#,%,&,etc).\n",
    "    \n",
    "    This method ensures to remove punctuation.\n",
    "    \"\"\"\n",
    "    temp_list, jargon_symbols = [], ('%','$','#','@','^','&','*','(',')','+','-','/','\\'','!','?','.',',',':',';','~','0','1','2','3','4','5','6','7','8','9')\n",
    "    for row in data:\n",
    "        filtered_words = [_replace(word.lower().translate(string.punctuation), symbols=jargon_symbols) for word in row if word.lower() not in stopwords.words('english')]          \n",
    "        temp_list.append(filtered_words)\n",
    "    #\n",
    "    new_list = []\n",
    "    for row in temp_list:\n",
    "        row_list = []\n",
    "        for word in row:\n",
    "            if word != \"\":\n",
    "                row_list.append(word)\n",
    "        new_list.append(row_list)\n",
    "    #\n",
    "    return new_list\n",
    "#\n",
    "cleaned_tokenized_train_X, cleaned_tokenized_train_y = remove_stop_words(data=tokenized_train_X), remove_stop_words(data=tokenized_train_y)\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(cleaned_tokenized_train_X[0:5]) + \"\\n\")\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(cleaned_tokenized_train_y[0:5]) + \"\\n\")\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(cleaned_tokenized_train_X)))\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(cleaned_tokenized_train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger: 87129.76521340893\n",
      "Anticipation: 246108.77993036134\n",
      "Disgust: 66221.3765328181\n",
      "Fear: 6178.9081192764515\n",
      "Joy: 144617.335856044\n",
      "Sadness: 113490.02210260557\n",
      "Surprise: 79.35627723628282\n",
      "Trust: 16098.18904906194\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes:\n",
    "    \"\"\"\n",
    "    Implements a hand built Naive Bayes superivised classifier on top of the BOW methodology,\n",
    "    along with predict and evaluation methods for the respective model built.\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, emotions, emotion_index):\n",
    "        self.BOW = {}\n",
    "        self.emotions = emotions\n",
    "        self.emotion_index = emotion_index\n",
    "        if self.emotion_index > len(self.emotions):\n",
    "            print(\"Index exceeds the current emotion list capacity. Index musn't exceed \" + str(len(self.emotions)))\n",
    "        self.likelihood_prob = None\n",
    "        self.emotion_count = 0\n",
    "        self.class_prior_prob = None\n",
    "    #\n",
    "    def __prod(self,iterable):\n",
    "        \"\"\"\n",
    "        Product multiplier\n",
    "        \"\"\"\n",
    "        total = 1\n",
    "        for number in iterable:\n",
    "            total *= number\n",
    "        return total\n",
    "    #\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        NB classifier function used to train the model. \n",
    "        Operates by constructing a bag of words model.\n",
    "        \"\"\"\n",
    "        selected_emotion = self.get_sentiment()\n",
    "        for i in range(len(y)):\n",
    "            if selected_emotion in y[i]:\n",
    "                self.emotion_count += 1\n",
    "                for word in X[i]:\n",
    "                    if word in self.BOW:\n",
    "                        self.BOW[word] += 1\n",
    "                    else:\n",
    "                        self.BOW[word] = 1\n",
    "        #\n",
    "        self.class_prior_prob = self.emotion_count / len(y)\n",
    "    #\n",
    "    def predict_proba(self,X):\n",
    "        \"\"\"\n",
    "        NB classifier function used to return probability estimates for the test vector.\n",
    "        \"\"\"\n",
    "        #\n",
    "        likelihood_probalities = []\n",
    "        for word in X:\n",
    "            if word in self.BOW:\n",
    "                likelihood_probalities.append((self.BOW[word] + alpha/self.emotion_count))\n",
    "        self.likelihood_prob = self.__prod(likelihood_probalities)\n",
    "        return self.likelihood_prob * self.class_prior_prob\n",
    "    #\n",
    "    def get_sentiment(self):\n",
    "        \"\"\"\n",
    "        Returns sentiment of this instance classifier\n",
    "        \"\"\"\n",
    "        return self.emotions[self.emotion_index]\n",
    "#\n",
    "NB_classifier_anger = NaiveBayes(emotions, 0)\n",
    "NB_classifier_anger.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_anticipation = NaiveBayes(emotions, 1)\n",
    "NB_classifier_anticipation.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_disgust = NaiveBayes(emotions, 2)\n",
    "NB_classifier_disgust.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_fear = NaiveBayes(emotions, 3)\n",
    "NB_classifier_fear.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_joy = NaiveBayes(emotions, 4)\n",
    "NB_classifier_joy.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_sadness = NaiveBayes(emotions, 5)\n",
    "NB_classifier_sadness.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_surprise = NaiveBayes(emotions, 6)\n",
    "NB_classifier_surprise.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_trust = NaiveBayes(emotions, 7)\n",
    "NB_classifier_trust.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "classifiers = [NB_classifier_anger, \n",
    "               NB_classifier_anticipation, \n",
    "               NB_classifier_disgust, \n",
    "               NB_classifier_fear, \n",
    "               NB_classifier_joy, \n",
    "               NB_classifier_sadness, \n",
    "               NB_classifier_surprise, \n",
    "               NB_classifier_trust]\n",
    "test = \"If you want to live as a healer\".split()\n",
    "pred = NB_classifier_anger.predict_proba(test)\n",
    "print(\"Anger: \" + str(pred))\n",
    "pred = NB_classifier_anticipation.predict_proba(test)\n",
    "print(\"Anticipation: \" + str(pred))\n",
    "pred = NB_classifier_disgust.predict_proba(test)\n",
    "print(\"Disgust: \" + str(pred))\n",
    "pred = NB_classifier_fear.predict_proba(test)\n",
    "print(\"Fear: \" + str(pred))\n",
    "pred = NB_classifier_joy.predict_proba(test)\n",
    "print(\"Joy: \" + str(pred))\n",
    "pred = NB_classifier_sadness.predict_proba(test)\n",
    "print(\"Sadness: \" + str(pred))\n",
    "pred = NB_classifier_surprise.predict_proba(test)\n",
    "print(\"Surprise: \" + str(pred))\n",
    "pred = NB_classifier_trust.predict_proba(test)\n",
    "print(\"Trust: \" + str(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of training corpus: \n",
      "[['exalts', 'shall', 'humbled', 'humbles', 'shall', 'exaltedmatt', 'semst'], ['rt', 'prayerbullets', 'remove', 'nehushtan', 'previous', 'moves', 'god', 'become', 'idols', 'high', 'places', 'kings', 'semst'], ['brainman', 'heidtjj', 'benjaminlives', 'sought', 'truth', 'soul', 'found', 'strong', 'enough', 'stand', 'merits', 'semst'], ['god', 'utterly', 'powerless', 'without', 'human', 'intervention', 'semst'], ['david_cameron', 'miracles', 'multiculturalism', 'miracles', 'shady', 'taqiya', 'tawriya', 'jaziya', 'kafirs', 'dhimmi', 'jihad', 'allah', 'semst']]\n",
      "\n",
      "First 5 lines of training corpus: \n",
      "[['anger', 'anticipation', 'disgust', 'joy', 'trust'], ['joy', 'sadness', 'trust'], ['anticipation', 'joy', 'trust'], ['anticipation', 'fear', 'sadness', 'trust'], ['anger', 'anticipation', 'fear', 'joy', 'surprise', 'trust']]\n",
      "\n",
      "Shape of training corpus: \n",
      "(1956,)\n",
      "Shape of training corpus: \n",
      "(1956,)\n",
      "\n",
      "--------------------------------------\n",
      "\n",
      "Scoring with AT MOST one match with test sample\n",
      "Accuracy: 71.881390593\n",
      "Precision: 100.0\n",
      "Recall: 71.881390593\n",
      "F-Score: 83.6406900654\n",
      "\n",
      "Scoring with AT LEAST one match with test sample\n",
      "Accuracy: 19.5603784085\n",
      "Precision: 100.0\n",
      "Recall: 19.5603784085\n",
      "F-Score: 32.7205026763\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Testing\n",
    "test_corpus = open_file_and_return_dump(path=test_path)\n",
    "test_y, test_X = split_labels_from_vectors(corpus=test_corpus)\n",
    "tokenized_test_X, tokenized_test_y = tokenize_corpus(data=test_X), tokenize_corpus(data=test_y, delimeter=\"\\t\")\n",
    "cleaned_tokenized_test_X, cleaned_tokenized_test_y = remove_stop_words(data=tokenized_test_X), remove_stop_words(data=tokenized_test_y)\n",
    "#\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(cleaned_tokenized_test_X[0:5]) + \"\\n\")\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(cleaned_tokenized_test_y[0:5]) + \"\\n\")\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(cleaned_tokenized_test_X)))\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(cleaned_tokenized_test_y)) + \"\\n\\n--------------------------------------\\n\")\n",
    "#\n",
    "def predict_overall_sentiment(classifiers, X):\n",
    "    \"\"\"\n",
    "    Accepts a list of NB classifiers, and returns the highest probabilistic sentiment\n",
    "    \"\"\"\n",
    "    sentiment, prob = 0, 0\n",
    "    for classifier in classifiers:\n",
    "        temp_prob = classifier.predict_proba(X=X)\n",
    "        if temp_prob > prob:\n",
    "            sentiment = classifier.get_sentiment()\n",
    "            prob = temp_prob\n",
    "    return sentiment, prob\n",
    "#\n",
    "y_true, y_pred = [], []\n",
    "for i in range(len(cleaned_tokenized_test_X)):\n",
    "    sentiment, prob = predict_overall_sentiment(classifiers=classifiers, X=cleaned_tokenized_test_X[i])\n",
    "    if sentiment in cleaned_tokenized_test_y[i]:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "    y_true.append(1)\n",
    "#\n",
    "# For this case, true positives are considered when at least a single element \n",
    "# of the predicted matches one of emotions in the test sample \n",
    "print(\"Scoring with AT MOST one match with test sample\")\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_true, y_pred) * 100))\n",
    "print(\"Precision: \" + str(precision_score(y_true, y_pred, average='binary') * 100))\n",
    "print(\"Recall: \" + str(recall_score(y_true, y_pred) * 100))\n",
    "print(\"F-Score: \" + str(f1_score(y_true, y_pred) * 100))\n",
    "#\n",
    "y_true, y_pred = [], []\n",
    "for i in range(len(cleaned_tokenized_test_X)):\n",
    "    sentiment, prob = predict_overall_sentiment(classifiers=classifiers, X=cleaned_tokenized_test_X[i])\n",
    "    for element in cleaned_tokenized_test_y[i]:\n",
    "        if sentiment == element:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "        y_true.append(1)\n",
    "#\n",
    "# For this case, true positives are considered by comapring the predicted to all the test labels. \n",
    "# Since we can only predict a single emotion at a single time, it is to be expected for overall\n",
    "# scoring to plummet when compared to the previous version\n",
    "print(\"\\nScoring with AT LEAST one match with test sample\")\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_true, y_pred) * 100))\n",
    "print(\"Precision: \" + str(precision_score(y_true, y_pred, average='binary') * 100))\n",
    "print(\"Recall: \" + str(recall_score(y_true, y_pred) * 100))\n",
    "print(\"F-Score: \" + str(f1_score(y_true, y_pred) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
