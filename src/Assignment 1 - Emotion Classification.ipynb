{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import string\n",
    "#\n",
    "class Document:\n",
    "    \"\"\"\n",
    "    This class oversees reading, cleaning and formatting of the text corpus\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, path=None):\n",
    "        if path is not None:\n",
    "            BASE_DIR = os.path.join( os.path.dirname(os.getcwd()))\n",
    "            self.path = BASE_DIR + path\n",
    "        # Emotion Categories (Classes)\n",
    "        self.emotions = (\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"trust\")\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    #\n",
    "    def __open_file_and_return_dump(self):\n",
    "        \"\"\" \n",
    "        Opens file and returns a dump containing all the data.\n",
    "        The data structure used to record the dumped data is a list of strings,\n",
    "        where each string represents a single record).\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        with open(self.path) as csvfile:\n",
    "            rows = csv.reader(csvfile)\n",
    "            for row in rows:\n",
    "                temp_string = \"\"\n",
    "                for item in row:\n",
    "                    temp_string += str(item) + \" \"\n",
    "                corpus.append(temp_string)\n",
    "        return corpus\n",
    "    #\n",
    "    def __split_labels_from_vectors(self):\n",
    "        \"\"\"\n",
    "        Takes corpus as input, and splits the data into respective labels and training data.\n",
    "        Returns this data as two separate lists\n",
    "        \"\"\"\n",
    "        #\n",
    "        corpus = self.__open_file_and_return_dump()\n",
    "        label_count = len(self.emotions)\n",
    "        character_splitter = '\\t'\n",
    "        train_y, train_X = [], []\n",
    "        for row in corpus:\n",
    "            groups = row.split(character_splitter)\n",
    "            train_y.append(character_splitter.join(groups[:label_count]))\n",
    "            train_X.append(character_splitter.join(groups[label_count:]))\n",
    "        return train_y, train_X\n",
    "    #\n",
    "    def __tokenize_corpus(self, data, delimeter=\" \"):\n",
    "        \"\"\"\n",
    "        Takes a list as input, and tokenizes the data.\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        [(corpus.append(row.split(delimeter))) for row in data]\n",
    "        return corpus\n",
    "    #\n",
    "    def __replace(self, word, symbols, placeholder=\"\"):\n",
    "        \"\"\"\n",
    "        An overriding of the original python method, so as to replace all characters \n",
    "        in a string based on whether they occur in a list.\n",
    "        \"\"\"\n",
    "        temp_string = word\n",
    "        for symbol in symbols:\n",
    "            temp_string = temp_string.replace(symbol, placeholder)\n",
    "        return temp_string\n",
    "    #\n",
    "    def __remove_stop_words(self, data):\n",
    "        \"\"\"\n",
    "        Takes a list of data, and iterates over each element to \n",
    "        scan and remove stop words.\n",
    "\n",
    "        The method ensures to convert all text instances to lowercase.\n",
    "\n",
    "        The method ensures to remove jargon symbols (#,%,&,etc).\n",
    "\n",
    "        This method ensures to remove punctuation.\n",
    "        \"\"\"\n",
    "        temp_list, jargon_symbols = [], ('%','$','#','@','^','&','*','(',')','+','/','\\'','!','?','.',',',':',';','~','0','1','2','3','4','5','6','7','8','9')\n",
    "        for row in data:\n",
    "            filtered_words = [self.__replace(word.lower().translate(string.punctuation), symbols=jargon_symbols) for word in row if word.lower() not in stopwords.words('english')]          \n",
    "            temp_list.append(filtered_words)\n",
    "        #\n",
    "        new_list = []\n",
    "        for row in temp_list:\n",
    "            row_list = []\n",
    "            for word in row:\n",
    "                if word != \"\":\n",
    "                    row_list.append(word)\n",
    "            new_list.append(row_list)\n",
    "        #\n",
    "        return new_list\n",
    "    #\n",
    "    def __word_morhpology(self, data, stemming=0, lemmatizing=0):\n",
    "        temp_list = []\n",
    "        for row in data:\n",
    "            filtered_words = row\n",
    "            if stemming == 1:\n",
    "                # Stemming\n",
    "                filtered_words = [self.stemmer.stem(word) for word in filtered_words]\n",
    "            #\n",
    "            if stemming == 1:\n",
    "                # Lemmatizing\n",
    "                filtered_words = [self.lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "            #\n",
    "            temp_list.append(filtered_words)\n",
    "        #\n",
    "        return temp_list\n",
    "    #\n",
    "    def get_emotions(self):\n",
    "        return self.emotions\n",
    "    #\n",
    "    def clean_corpus(self):\n",
    "        train_y, train_X = self.__split_labels_from_vectors()\n",
    "        tokenized_X, tokenized_y = self.__tokenize_corpus(data=train_X), self.__tokenize_corpus(data=train_y, delimeter=\"\\t\")\n",
    "        cleaned_tokenized_X, cleaned_tokenized_y = self.__remove_stop_words(data=tokenized_X), self.__remove_stop_words(data=tokenized_y)\n",
    "        cleaned_tokenized_X = self.__word_morhpology(cleaned_tokenized_X,1,1)\n",
    "        return cleaned_tokenized_X, cleaned_tokenized_y\n",
    "    #\n",
    "    def binarize(self, y):\n",
    "        value = \"---\"\n",
    "        y_binarized = []\n",
    "        for i in range(len(y)):\n",
    "            y_binarized.append([])\n",
    "            for element in y[i]:\n",
    "                if value in element:\n",
    "                    y_binarized[i].append(0)\n",
    "                else:\n",
    "                    y_binarized[i].append(1)\n",
    "        return y_binarized\n",
    "#\n",
    "training_document = Document(path=\"\\\\data\\\\ssec-aggregated\\\\train-combined-0.0.csv\")\n",
    "cleaned_tokenized_train_X, cleaned_tokenized_train_y = training_document.clean_corpus()\n",
    "#\n",
    "# This will be used later for evaluation purposes\n",
    "testing_document = Document(path= \"\\\\data\\\\ssec-aggregated\\\\test-combined-0.0.csv\")\n",
    "cleaned_tokenized_test_X, cleaned_tokenized_test_y = testing_document.clean_corpus()\n",
    "binarized_cleaned_tokenized_test_y = testing_document.binarize(cleaned_tokenized_test_y)\n",
    "#\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(cleaned_tokenized_train_X[0:5]) + \"\\n\")\n",
    "print(\"First 5 lines of training corpus: \\n\" + str(cleaned_tokenized_train_y[0:5]) + \"\\n\")\n",
    "print(\"First 5 lines of testing corpus: \\n\" + str(cleaned_tokenized_test_y[0:5]) + \"\\n\")\n",
    "print(\"First 5 lines of testing corpus: \\n\" + str(binarized_cleaned_tokenized_test_y[0:5]) + \"\\n\")\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(cleaned_tokenized_train_X)))\n",
    "print(\"Shape of training corpus: \\n\" + str(np.shape(cleaned_tokenized_train_y)))\n",
    "print(\"Shape of testing corpus: \\n\" + str(np.shape(cleaned_tokenized_test_y)))\n",
    "print(\"Shape of testing corpus: \\n\" + str(np.shape(binarized_cleaned_tokenized_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \"\"\"\n",
    "    This class oversees the Naive Bayes Model Template\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, emotion_index):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        d = Document()\n",
    "        self.emotions = d.get_emotions()\n",
    "        self.emotion_index = emotion_index\n",
    "        self.alpha = 1 # Laplace Smoothener\n",
    "        if self.emotion_index > len(self.emotions):\n",
    "            print(\"Index exceeds the current emotion list capacity. Index musn't exceed \" + str(len(self.emotions)))\n",
    "        self.BOW_0, self.BOW_1 = {}, {} # Separate BOW models (BOW_1 - For likely vocab, BOW_0 - For not likely vocab)\n",
    "        self.word_count_category_0, self.word_count_category_1 = 0,0 \n",
    "    #\n",
    "    def __get_sentiment(self):\n",
    "        \"\"\"\n",
    "        Returns sentiment of this instance classifier\n",
    "        \"\"\"\n",
    "        return self.emotions[self.emotion_index]\n",
    "    #\n",
    "    def __prod(self,iterable):\n",
    "        \"\"\"\n",
    "        Product multiplier - Accepts list and returns product of all list elements\n",
    "        \"\"\"\n",
    "        total = 1\n",
    "        for number in iterable:\n",
    "            total *= number\n",
    "        return total\n",
    "    #\n",
    "    def __calculate_NB(self, liklihood_prob, prior_prob):\n",
    "        return self.__prod(liklihood_prob) * prior_prob\n",
    "    #\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Accepts training data and training labels, which will form the basis of the Naive Bayes model.\n",
    "        This function oversees cosntruction of the bag of words model. \n",
    "        \"\"\"\n",
    "        selected_emotion = self.__get_sentiment()\n",
    "        #\n",
    "        for i in range(len(y)):\n",
    "            if selected_emotion in y[i]:\n",
    "                for word in X[i]:\n",
    "                    self.word_count_category_1 += 1\n",
    "                    if word in self.BOW_1:\n",
    "                        self.BOW_1[word] += 1\n",
    "                    else:\n",
    "                        self.BOW_1[word] = 1\n",
    "            else:\n",
    "                for word in X[i]:\n",
    "                    self.word_count_category_0 += 1\n",
    "                    if word in self.BOW_0:\n",
    "                        self.BOW_0[word] += 1\n",
    "                    else:\n",
    "                        self.BOW_0[word] = 1\n",
    "    #\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Accepts input document, and returns estimated liklihoods for an emotion holding and not holding\n",
    "        \"\"\"\n",
    "        prob_0, prob_1 = 0,0\n",
    "        #\n",
    "        # Calculating the liklihood probability of having this emotion\n",
    "        likelihood_probabilities = []\n",
    "        for word in X:\n",
    "            if word in self.BOW_1:\n",
    "                likelihood_probability = (self.BOW_1[word] + self.alpha) / (self.word_count_category_1 + len(self.BOW_1))\n",
    "                likelihood_probabilities.append(likelihood_probability)\n",
    "        prior_prob = sum(self.BOW_1.values()) / (sum(self.BOW_0.values()) + sum(self.BOW_1.values()))\n",
    "        prob_1 = self.__calculate_NB(likelihood_probabilities, prior_prob)\n",
    "        #\n",
    "        # Calculating the liklihood probability of not having this emotion\n",
    "        likelihood_probabilities = []\n",
    "        for word in X:\n",
    "            if word in self.BOW_0:\n",
    "                likelihood_probability = (self.BOW_0[word] + self.alpha) / (self.word_count_category_0 + len(self.BOW_0))\n",
    "                likelihood_probabilities.append(likelihood_probability)\n",
    "        prior_prob = sum(self.BOW_0.values()) / (sum(self.BOW_0.values()) + sum(self.BOW_1.values()))\n",
    "        prob_0 = self.__calculate_NB(likelihood_probabilities, prior_prob)\n",
    "        #\n",
    "        return prob_0, prob_1\n",
    "    #\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        A wrapper function for: \n",
    "        \n",
    "        self.predict_proba\n",
    "        \n",
    "        Returns whether emotion holds or not\n",
    "        \"\"\"\n",
    "        prob_0, prob_1 = self.predict_proba(X)\n",
    "        if prob_0 > prob_1:\n",
    "            return 0 # Emotion is not likely\n",
    "        else:\n",
    "            return 1 # Emotion is likely\n",
    "#\n",
    "NB_classifier_anger = NaiveBayes(0)\n",
    "NB_classifier_anger.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_anticipation = NaiveBayes(1)\n",
    "NB_classifier_anticipation.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_disgust = NaiveBayes(2)\n",
    "NB_classifier_disgust.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_fear = NaiveBayes(3)\n",
    "NB_classifier_fear.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_joy = NaiveBayes(4)\n",
    "NB_classifier_joy.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_sadness = NaiveBayes(5)\n",
    "NB_classifier_sadness.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_surprise = NaiveBayes(6)\n",
    "NB_classifier_surprise.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "NB_classifier_trust = NaiveBayes(7)\n",
    "NB_classifier_trust.fit(X=cleaned_tokenized_train_X, y=cleaned_tokenized_train_y)\n",
    "#\n",
    "classifiers = [NB_classifier_anger, \n",
    "               NB_classifier_anticipation, \n",
    "               NB_classifier_disgust, \n",
    "               NB_classifier_fear, \n",
    "               NB_classifier_joy, \n",
    "               NB_classifier_sadness, \n",
    "               NB_classifier_surprise, \n",
    "               NB_classifier_trust]\n",
    "#\n",
    "# Quick Testing\n",
    "test = \"If you want to live as a healer\".split()\n",
    "pred = NB_classifier_anger.predict(test)\n",
    "print(\"Anger: \" + str(pred))\n",
    "pred = NB_classifier_anticipation.predict(test)\n",
    "print(\"Anticipation: \" + str(pred))\n",
    "pred = NB_classifier_disgust.predict(test)\n",
    "print(\"Disgust: \" + str(pred))\n",
    "pred = NB_classifier_fear.predict(test)\n",
    "print(\"Fear: \" + str(pred))\n",
    "pred = NB_classifier_joy.predict(test)\n",
    "print(\"Joy: \" + str(pred))\n",
    "pred = NB_classifier_sadness.predict(test)\n",
    "print(\"Sadness: \" + str(pred))\n",
    "pred = NB_classifier_surprise.predict(test)\n",
    "print(\"Surprise: \" + str(pred))\n",
    "pred = NB_classifier_trust.predict(test)\n",
    "print(\"Trust: \" + str(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = binarized_cleaned_tokenized_test_y, []\n",
    "for sentence in cleaned_tokenized_test_X:\n",
    "    predictions = [NB_classifier_anger.predict(sentence),\n",
    "                   NB_classifier_anticipation.predict(sentence),\n",
    "                   NB_classifier_disgust.predict(sentence),\n",
    "                   NB_classifier_fear.predict(sentence),\n",
    "                   NB_classifier_joy.predict(sentence),\n",
    "                   NB_classifier_sadness.predict(sentence),\n",
    "                   NB_classifier_surprise.predict(sentence),\n",
    "                   NB_classifier_trust.predict(sentence),]\n",
    "    y_pred.append(predictions)\n",
    "#\n",
    "def flatten(iterable):\n",
    "    flattened_list = []\n",
    "    for row in iterable:\n",
    "        for element in row:\n",
    "            flattened_list.append(element)\n",
    "    return flattened_list\n",
    "y_true = flatten(y_true)\n",
    "y_pred = flatten(y_pred)\n",
    "#\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_true, y_pred) * 100))\n",
    "print(\"Precision: \" + str(precision_score(y_true, y_pred) * 100))\n",
    "print(\"Recall: \" + str(recall_score(y_true, y_pred) * 100))\n",
    "print(\"F-Score: \" + str(f1_score(y_true, y_pred) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
